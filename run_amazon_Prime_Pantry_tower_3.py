import torch.optim as optim
import re
from pathlib import Path
from torch.utils.data import DataLoader
import numpy as np
from transformers import BertModel, BertTokenizer, BertConfig, \
    RobertaTokenizer, RobertaModel, RobertaConfig, \
    DebertaTokenizer, DebertaModel, DebertaConfig, \
    DistilBertTokenizer, DistilBertModel, DistilBertConfig, \
    GPT2Tokenizer, OPTModel, OPTConfig

from parameters import parse_args
from model.model_mmd_ada import Model_new3_3, Model2, Model2_align, Model2_transfer, Bert_Encoder
from data_utils import eval_model_2_3tower_amazon_pantry, BuildTrainDataset_new_amazon_pantry, \
    get_item_embeddings_llm_3tower, eval_model_2_3tower_amazon, eval_model_2_2_amazon, BuildTrainDataset_new_amazon_ele, \
    read_news, read_news_bert, get_doc_input_bert, get_id_embeddings_amazon, \
    read_behaviors, BuildTrainDataset, eval_model_amazon, eval_model, eval_model_step2, get_item_embeddings, \
    get_item_embeddings_llm, get_item_word_embs, get_item_word_embs_llm, get_item_embeddings_llm_4
from data_utils import read_news_bert_amazon_pantry, read_behaviors_amazon_pantry
from data_utils.utils import *
import random

import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.nn.init import xavier_normal_
import gc
import joblib

# ç¦ç”¨tokenizerså¹¶è¡Œå¤„ç†ï¼Œé¿å…ä¸åˆ†å¸ƒå¼è®­ç»ƒå†²çª
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# å®šä¹‰æ•°æ®é›†åç§°å’Œé¢„è®­ç»ƒçš„LLMåµŒå…¥æ–‡ä»¶å
datasets = "Prime_Pantry"
llm_embedding = "Prime_Pantry_llm2vec.pt"
early_stop = 50


def train(args, use_modal, local_rank):
    """
    ä¸»è®­ç»ƒå‡½æ•°ï¼Œå®ç°äº†åŸºäºTransformerçš„æ¨èç³»ç»Ÿæ¨¡å‹è®­ç»ƒè¿‡ç¨‹

    å‚æ•°:
        args: å‘½ä»¤è¡Œè§£æçš„å‚æ•°å¯¹è±¡
        use_modal: æ˜¯å¦ä½¿ç”¨æ¨¡æ€ä¿¡æ¯ï¼ˆå¦‚æ–‡æœ¬ç‰¹å¾ï¼‰
        local_rank: å½“å‰è¿›ç¨‹çš„æœ¬åœ°GPUåºå·
    """
    global item_num, users_train, item_word_embs
    if use_modal:
        # æ ¹æ®å‘½ä»¤è¡Œå‚æ•°åŠ è½½ä¸åŒçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹
        if 'roberta' in args.bert_model_load:
            Log_file.info('load roberta model...')
            bert_model_load = '../../pretrained_models/' + args.bert_model_load
            tokenizer = RobertaTokenizer.from_pretrained(bert_model_load)
            config = RobertaConfig.from_pretrained(bert_model_load, output_hidden_states=True)
            bert_model = RobertaModel.from_pretrained(bert_model_load, config=config)
            # è®¾ç½®è¯åµŒå…¥ç»´åº¦
            if 'base' in args.bert_model_load:
                args.word_embedding_dim = 768
            if 'large' in args.bert_model_load:
                args.word_embedding_dim = 1024
        elif 'opt' in args.bert_model_load:
            Log_file.info('load opt model...')
            bert_model_load = '../../pretrained_models/' + args.bert_model_load
            tokenizer = GPT2Tokenizer.from_pretrained(bert_model_load)
            config = OPTConfig.from_pretrained(bert_model_load, output_hidden_states=True)
            bert_model = OPTModel.from_pretrained(bert_model_load, config=config)
        elif 'llm' in args.bert_model_load:
            Log_file.info('load llm2vec...')
            args.word_embedding_dim = 4096  # LLMåµŒå…¥ç»´åº¦ä¸º4096

        # è¯»å–å•†å“æ•°æ®
        Log_file.info('read news...')
        before_item_id_to_dic, before_item_name_to_id, before_item_id_to_name = read_news_bert_amazon_pantry(
            os.path.join(args.root_data_dir, args.dataset, args.news), args)

        # è¯»å–ç”¨æˆ·è¡Œä¸ºæ•°æ®
        Log_file.info('read behaviors...')
        item_num, item_id_to_dic, users_train, users_valid, users_test, \
            users_history_for_valid, users_history_for_test, item_name_to_id = \
            read_behaviors_amazon_pantry(os.path.join(args.root_data_dir, args.dataset, args.behaviors),
                                         before_item_id_to_dic,
                                         before_item_name_to_id, before_item_id_to_name,
                                         args.max_seq_len, args.min_seq_len, Log_file)
        Log_file.info('Finish reading behaviors')

        # åŠ è½½é¢„è®­ç»ƒçš„å•†å“åµŒå…¥å‘é‡
        item_word_embs = torch.load(f'./dataset/{datasets}/{llm_embedding}')
        item_word_embs = torch.tensor(item_word_embs, dtype=torch.float32)
        Log_file.info('Finish reading item embeddings')

    Log_file.info('build dataset...')

    # è®¾ç½®å•†å“æ•°é‡
    # item_num = 8347
    # æ„å»ºè®­ç»ƒæ•°æ®é›†å¯¹è±¡
    train_dataset = BuildTrainDataset_new_amazon_pantry(u2seq=users_train, item_content=item_word_embs,
                                                        item_num=item_num,
                                                        max_seq_len=args.max_seq_len, use_modal=use_modal)
    Log_file.info('build dataset done...')
    len_users_train = len(users_train)
    del users_train  # é‡Šæ”¾å†…å­˜
    gc.collect()

    # æ„å»ºåˆ†å¸ƒå¼é‡‡æ ·å™¨
    Log_file.info('build DDP sampler...')
    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
    Log_file.info('before seed')

    # å®šä¹‰å·¥ä½œçº¿ç¨‹çš„éšæœºç§å­åˆå§‹åŒ–å‡½æ•°ï¼Œç¡®ä¿åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„éšæœºæ€§æ˜¯å¯å¤ç°çš„
    def worker_init_reset_seed(worker_id):
        initial_seed = torch.initial_seed() % 2 ** 31
        worker_seed = initial_seed + worker_id + dist.get_rank()
        random.seed(worker_seed)
        np.random.seed(worker_seed)

    # æ„å»ºæ•°æ®åŠ è½½å™¨
    Log_file.info('build dataloader...')
    train_dl = DataLoader(train_dataset, batch_size=args.batch_size, num_workers=args.num_workers,
                          worker_init_fn=worker_init_reset_seed, pin_memory=True, sampler=train_sampler)

    # æ„å»ºæ¨¡å‹
    Log_file.info('build model...')
    model = Model_new3_3(args, item_num, use_modal).to(local_rank)
    # å°†æ¨¡å‹ä¸­çš„æ™®é€šBatchNormè½¬æ¢ä¸ºåŒæ­¥BatchNormï¼Œä»¥ä¾¿åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­æ­£ç¡®è®¡ç®—æ‰¹å½’ä¸€åŒ–ç»Ÿè®¡é‡
    model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(local_rank)

    if use_modal:
        # è¾“å‡ºæ¨¡å‹ä¸­çš„å…³é”®ç»„ä»¶ä¿¡æ¯
        Log_file.info(model.turn_dim1)
        Log_file.info(model.fc)
        Log_file.info(model.mlp_layers)

    # å¦‚æœæŒ‡å®šäº†åŠ è½½æ£€æŸ¥ç‚¹ï¼Œåˆ™ä»æ£€æŸ¥ç‚¹æ¢å¤æ¨¡å‹çŠ¶æ€
    if 'None' not in args.load_ckpt_name:
        Log_file.info('load ckpt if not None...')
        ckpt_path = get_checkpoint(item_emb_path, args.load_ckpt_name)
        checkpoint = torch.load(ckpt_path, map_location=torch.device('cpu'))
        Log_file.info('load checkpoint...')
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        Log_file.info(f"Model loaded from {ckpt_path}")
        # ä»æ£€æŸ¥ç‚¹åç§°ä¸­è§£æèµ·å§‹è½®æ¬¡
        start_epoch = int(re.split(r'[._-]', args.load_ckpt_name)[1])
        # æ¢å¤éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€ï¼Œç¡®ä¿è®­ç»ƒçš„éšæœºæ€§æ˜¯å¯å¤ç°çš„
        try:
            # å°è¯•æ¢å¤ CUDA éšæœºæ•°çŠ¶æ€
            torch.cuda.set_rng_state(checkpoint['cuda_rng_state'])
            print("âœ… CUDA RNG çŠ¶æ€æ¢å¤æˆåŠŸ")

        except RuntimeError as e:
            # å¦‚æœå¤±è´¥ï¼Œä¼˜é›…åœ°å¤„ç†é”™è¯¯
            print(f"âš ï¸ è·³è¿‡ CUDA RNG çŠ¶æ€æ¢å¤: {e}")
            print("ğŸ’¡ è¿™ä¸ä¼šå½±å“æ¨¡å‹æƒé‡ï¼Œåªæ˜¯éšæœºæ•°åºåˆ—å¯èƒ½ä¸åŒ")

        # torch.cuda.set_rng_state(checkpoint['cuda_rng_state'])
        is_early_stop = True
        model.freeze6()  # å†»ç»“éƒ¨åˆ†æ¨¡å‹å‚æ•°
    else:
        checkpoint = None  # æ–°è®­ç»ƒ
        ckpt_path = None  # æ–°è®­ç»ƒ
        start_epoch = 0
        is_early_stop = True

    # å°†æ¨¡å‹åŒ…è£…ä¸ºåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œæ¨¡å‹
    model = DDP(model, device_ids=[local_rank], output_device=local_rank, find_unused_parameters=False)

    # å°†æ¨¡å‹å‚æ•°åˆ†ä¸ºä¸¤ç»„ï¼š1) alphaå’Œbetaå‚æ•°ï¼Œ2) å…¶ä»–å‚æ•°
    # è¿™æ ·å¯ä»¥ä¸ºä¸åŒç»„çš„å‚æ•°è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡å’Œæƒé‡è¡°å‡
    w_params = [param for name, param in model.named_parameters() if 'alpha' in name or 'beta' in name]
    b_params = [param for name, param in model.named_parameters() if not 'alpha' in name and not 'beta' in name]
    optimizer = optim.AdamW(
        [{'params': w_params, 'lr': args.lr}, {'params': b_params, 'lr': args.lr, 'weight_decay': args.l2_weight}])

    # è¾“å‡ºæ¨¡å‹å‚æ•°ä¿¡æ¯
    total_num = sum(p.numel() for p in model.parameters())
    trainable_num = sum(p.numel() for p in model.parameters() if p.requires_grad)
    Log_file.info("##### total_num {} #####".format(total_num))
    Log_file.info("##### trainable_num {} #####".format(trainable_num))

    # å¼€å§‹è®­ç»ƒè¿‡ç¨‹
    Log_file.info('\n')
    Log_file.info('Training...')
    next_set_start_time = time.time()
    max_epoch, early_stop_epoch = 0, args.epoch
    max_eval_value, early_stop_count = 0, 0
    # è®¡ç®—æ—¥å¿—è®°å½•å’Œè¯„ä¼°çš„æ­¥éª¤é—´éš”
    steps_for_log, steps_for_eval = para_and_log(model, len_users_train, args.batch_size, Log_file,
                                                 logging_num=args.logging_num, testing_num=args.testing_num)
    # åˆ›å»ºæ··åˆç²¾åº¦è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾å™¨
    scaler = torch.cuda.amp.GradScaler()
    if 'None' not in args.load_ckpt_name:
        scaler.load_state_dict(checkpoint["scaler_state"])
        Log_file.info(f"scaler loaded from {ckpt_path}")

    # åœ¨å±å¹•ä¸Šè¾“å‡ºè®­ç»ƒå¼€å§‹ä¿¡æ¯
    Log_screen.info('{} train start'.format(args.label_screen))
    # å¼€å§‹è®­ç»ƒå¾ªç¯
    for ep in range(args.epoch):
        now_epoch = start_epoch + ep + 1
        Log_file.info('\n')
        Log_file.info('epoch {} start'.format(now_epoch))
        Log_file.info('')
        loss, batch_index, need_break = 0.0, 1, False
        model.train()  # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        train_dl.sampler.set_epoch(now_epoch)  # è®¾ç½®é‡‡æ ·å™¨çš„è½®æ¬¡ï¼Œç¡®ä¿ä¸åŒè½®æ¬¡çš„æ•°æ®é¡ºåºä¸åŒ

        # éå†æ•°æ®åŠ è½½å™¨ä¸­çš„æ¯ä¸ªæ‰¹æ¬¡
        for data in train_dl:
            sample_items_id, sample_items_content, log_mask, bin_pos, bin_neg = data
            # å°†æ•°æ®ç§»åŠ¨åˆ°GPU
            sample_items_id, sample_items_content, log_mask, bin_pos, bin_neg = \
                sample_items_id.to(local_rank), sample_items_content.to(local_rank), log_mask.to(
                    local_rank), bin_pos.to(local_rank), bin_neg.to(local_rank)

            # é‡å¡‘è¾“å…¥å½¢çŠ¶
            if use_modal:
                sample_items_content = sample_items_content.view(-1, sample_items_content.size(-1))
            sample_items_id = sample_items_id.view(-1)

            # æ¸…é™¤ä¼˜åŒ–å™¨ä¸­çš„æ¢¯åº¦
            optimizer.zero_grad()
            # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            with torch.amp.autocast(device_type='cuda'):
                # å‰å‘ä¼ æ’­è®¡ç®—æŸå¤±
                bz_loss = model(sample_items_id, sample_items_content, log_mask, bin_pos, bin_neg, local_rank)
                loss += bz_loss.data.float()
            # ä½¿ç”¨æ¢¯åº¦ç¼©æ”¾è¿›è¡Œåå‘ä¼ æ’­å’Œä¼˜åŒ–å™¨æ›´æ–°
            scaler.scale(bz_loss).backward()
            scaler.step(optimizer)
            scaler.update()

            # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaNï¼Œå¦‚æœæ˜¯åˆ™ä¸­æ–­è®­ç»ƒ
            if torch.isnan(loss.data):
                need_break = True
                break

            # è®°å½•è®­ç»ƒæ—¥å¿—
            if batch_index % steps_for_log == 0:
                Log_file.info('cnt: {}, Ed: {}, batch loss: {:.5f}, sum loss: {:.5f}'.format(
                    batch_index, batch_index * args.batch_size, loss.data / batch_index, loss.data))
            batch_index += 1

        # å®šæœŸè¯„ä¼°æ¨¡å‹æ€§èƒ½
        if not need_break and now_epoch % 1 == 0:
            Log_file.info('')
            # è¿è¡ŒéªŒè¯é›†è¯„ä¼°ï¼Œå¹¶æ ¹æ®ç»“æœå†³å®šæ˜¯å¦æå‰åœæ­¢è®­ç»ƒ
            max_eval_value, max_epoch, early_stop_epoch, early_stop_count, need_break, need_save = \
                run_eval(now_epoch, max_epoch, early_stop_epoch, max_eval_value, early_stop_count,
                         model, item_word_embs, users_history_for_valid, users_valid, args.batch_size, item_num,
                         use_modal,
                         args.mode, is_early_stop, local_rank)
            model.train()  # è¯„ä¼°åå°†æ¨¡å‹è®¾å›è®­ç»ƒæ¨¡å¼
            # å¦‚æœæ¨¡å‹æ€§èƒ½æå‡ï¼Œåˆ™ä¿å­˜æ¨¡å‹
            if need_save and dist.get_rank() == 0:
                save_model(now_epoch, model, model_dir, optimizer,
                           torch.get_rng_state(), torch.cuda.get_rng_state(), scaler, Log_file)
        Log_file.info('')
        # è¾“å‡ºæœ¬è½®è®­ç»ƒçš„æ—¶é—´ä¿¡æ¯
        next_set_start_time = report_time_train(batch_index, now_epoch, loss, next_set_start_time, start_time, Log_file)
        Log_screen.info('{} training: epoch {}/{}'.format(args.label_screen, now_epoch, args.epoch))
        if need_break:
            break

    # åœ¨è®­ç»ƒç»“æŸæ—¶ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if dist.get_rank() == 0:
        save_model(now_epoch, model, model_dir, optimizer,
                   torch.get_rng_state(), torch.cuda.get_rng_state(), scaler, Log_file)

    # è¾“å‡ºè®­ç»ƒç»“æœæ‘˜è¦
    Log_file.info('\n')
    Log_file.info('%' * 90)
    Log_file.info(' max eval Hit10 {:0.5f}  in epoch {}'.format(max_eval_value * 100, max_epoch))
    Log_file.info(' early stop in epoch {}'.format(early_stop_epoch))
    Log_file.info('the End')
    Log_screen.info('{} train end in epoch {}'.format(args.label_screen, early_stop_epoch))
    Log_file.info('gamma2 {}'.format(args.gamma2))
    Log_file.info('lr {}'.format(args.lr))

    # åœ¨æµ‹è¯•é›†ä¸Šè¿›è¡Œæœ€ç»ˆè¯„ä¼°
    item_embeddings3, item_embeddings, id_embs = get_item_embeddings_llm_3tower(model, item_word_embs, args.batch_size,
                                                                                args, use_modal, local_rank)
    valid_Hit10 = eval_model_2_3tower_amazon_pantry(10, model, users_history_for_test, users_test, item_embeddings3,
                                                    item_embeddings, id_embs, 512, args,
                                                    item_num, Log_file, args.mode, local_rank)


def run_eval(now_epoch, max_epoch, early_stop_epoch, max_eval_value, early_stop_count,
             model, item_word_embs, user_history, users_eval, batch_size, item_num, use_modal,
             mode, is_early_stop, local_rank):
    """
    åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½

    å‚æ•°:
        now_epoch: å½“å‰è®­ç»ƒè½®æ¬¡
        max_epoch: æœ€ä½³æ€§èƒ½è½®æ¬¡
        early_stop_epoch: æå‰åœæ­¢çš„è½®æ¬¡
        max_eval_value: æœ€ä½³éªŒè¯é›†æ€§èƒ½
        early_stop_count: æ€§èƒ½æœªæå‡çš„è¿ç»­è½®æ¬¡è®¡æ•°
        model: è¦è¯„ä¼°çš„æ¨¡å‹
        item_word_embs: å•†å“æ–‡æœ¬åµŒå…¥
        user_history: ç”¨æˆ·å†å²è¡Œä¸º
        users_eval: ç”¨äºè¯„ä¼°çš„ç”¨æˆ·æ•°æ®
        batch_size: æ‰¹æ¬¡å¤§å°
        item_num: å•†å“æ•°é‡
        use_modal: æ˜¯å¦ä½¿ç”¨æ¨¡æ€ä¿¡æ¯
        mode: è¿è¡Œæ¨¡å¼
        is_early_stop: æ˜¯å¦å¯ç”¨æå‰åœæ­¢
        local_rank: å½“å‰è¿›ç¨‹çš„GPUåºå·

    è¿”å›:
        max_eval_value: æ›´æ–°åçš„æœ€ä½³éªŒè¯é›†æ€§èƒ½
        max_epoch: æ›´æ–°åçš„æœ€ä½³æ€§èƒ½è½®æ¬¡
        early_stop_epoch: æ›´æ–°åçš„æå‰åœæ­¢è½®æ¬¡
        early_stop_count: æ›´æ–°åçš„æ€§èƒ½æœªæå‡è®¡æ•°
        need_break: æ˜¯å¦éœ€è¦ä¸­æ–­è®­ç»ƒ
        need_save: æ˜¯å¦éœ€è¦ä¿å­˜æ¨¡å‹
    """
    eval_start_time = time.time()
    Log_file.info('Validating...')
    # è·å–å•†å“åµŒå…¥å‘é‡
    item_embeddings3, item_embeddings, id_embs = get_item_embeddings_llm_3tower(model, item_word_embs, batch_size, args,
                                                                                use_modal, local_rank)
    # è®¡ç®—Hit@10æŒ‡æ ‡
    valid_Hit10 = eval_model_2_3tower_amazon_pantry(10, model, user_history, users_eval, item_embeddings3,
                                                    item_embeddings, id_embs, 512, args, item_num, Log_file, mode,
                                                    local_rank)
    # è®°å½•è¯„ä¼°æ—¶é—´
    report_time_eval(eval_start_time, Log_file)
    Log_file.info('')

    need_break = False
    need_save = False
    # åˆ¤æ–­æ¨¡å‹æ€§èƒ½æ˜¯å¦æå‡
    if valid_Hit10 > max_eval_value:
        max_eval_value = valid_Hit10
        max_epoch = now_epoch
        early_stop_count = 0
        need_save = True
    else:
        early_stop_count += 1
        # å¦‚æœè¿ç»­20è½®æ€§èƒ½æœªæå‡ï¼Œåˆ™è€ƒè™‘æå‰åœæ­¢
        if early_stop_count > early_stop:
            if is_early_stop:
                need_break = True
            early_stop_epoch = now_epoch
    return max_eval_value, max_epoch, early_stop_epoch, early_stop_count, need_break, need_save


def setup_seed(seed):
    """
    è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒå¯å¤ç°æ€§

    å‚æ•°:
        seed: éšæœºç§å­å€¼
    """
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    args = parse_args()
    # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ
    dist.init_process_group(backend='nccl')
    local_rank = torch.distributed.get_rank()
    torch.cuda.set_device(local_rank)
    # è®¾ç½®éšæœºç§å­
    setup_seed(12345)
    gpus = torch.cuda.device_count()
    early_stop = args.early_stop

    item_emb = f"Tower_2_{args.embedding_dim}"
    # æ ¹æ®æ¨¡å‹æ¶æ„é€‰æ‹©ç¡®å®šæ˜¯å¦ä½¿ç”¨æ¨¡æ€ä¿¡æ¯
    if 'modal' in args.item_tower:
        is_use_modal = True
        model_load = '/'
        flag = 0.0001
        # è®¾ç½®ç›®å½•æ ‡ç­¾å’Œæ—¥å¿—å‚æ•°
        tower_name = f"Tower_{args.tower}_{args.embedding_dim}"
        dir_label = os.path.join(args.dataset, tower_name)
        # æ„å»ºåŒ…å«å…¶ä»–å‚æ•°çš„å­ç›®å½•æˆ–æ–‡ä»¶å
        log_paras = f"bs{args.batch_size}_lr{args.lr}_modnn{args.mo_dnn_layers}_dnn{args.dnn_layers}"
    else:
        is_use_modal = False
        # è®¾ç½®ç›®å½•æ ‡ç­¾å’Œæ—¥å¿—å‚æ•°
        tower_name = f"Tower_{args.tower}_{args.embedding_dim}"
        dir_label = os.path.join(args.dataset, tower_name)
        # æ„å»ºåŒ…å«å…¶ä»–å‚æ•°çš„å­ç›®å½•æˆ–æ–‡ä»¶å
        log_paras = f"bs{args.batch_size}_lr{args.lr}_modnn{args.mo_dnn_layers}_dnn{args.dnn_layers}"

    # é¡¹ç›®çš„IDåµŒå…¥
    item_emb_path = os.path.join('./checkpoint', args.dataset, item_emb)
    # è®¾ç½®æ¨¡å‹ä¿å­˜è·¯å¾„
    model_dir = os.path.join('./checkpoint', dir_label)
    # ç”Ÿæˆæ—¶é—´æˆ³ï¼Œç”¨äºæ ‡è®°æ—¥å¿—æ–‡ä»¶
    time_run = time.strftime('-%Y%m%d-%H%M%S', time.localtime())
    args.label_screen = args.label_screen + time_run

    # è®¾ç½®æ—¥å¿—è®°å½•å™¨
    Log_file, Log_screen = setuplogger(dir_label, log_paras, time_run, args.mode, dist.get_rank(), args.behaviors)
    Log_file.info(args)
    # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
    if not os.path.exists(model_dir):
        Path(model_dir).mkdir(parents=True, exist_ok=True)

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    # æ ¹æ®æ¨¡å¼è¿è¡Œè®­ç»ƒæˆ–è¯„ä¼°
    if 'train' in args.mode:
        print(local_rank)
        train(args, is_use_modal, local_rank)
    # è®°å½•ç»“æŸæ—¶é—´å¹¶è¾“å‡ºæ€»è€—æ—¶
    end_time = time.time()
    hour, minu, secon = get_time(start_time, end_time)
    Log_file.info("##### (time) all: {} hours {} minutes {} seconds #####".format(hour, minu, secon))
    print(args.gamma)
    Log_file.info("##### freeze: {} gamma: {}".format(args.freeze, args.gamma))
