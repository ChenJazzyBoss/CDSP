import torch
import torch.nn as nn
from torch.nn.init import xavier_normal_, constant_
from .modules import TransformerEncoder

# PCAæ”¯æŒæ£€æŸ¥
try:
    from sklearn.decomposition import PCA

    PCA_AVAILABLE = True
except ImportError:
    PCA_AVAILABLE = False


# ========== KAN åº“å¯¼å…¥æ£€æŸ¥ ==========
def try_import_efficient_kan():
    """æ™ºèƒ½å¯¼å…¥ efficient-kan"""
    try:
        from efficient_kan import KANLinear
        print("âœ… efficient-kan å¯¼å…¥æˆåŠŸ")
        return KANLinear, True
    except ImportError:
        print("âŒ efficient-kan æœªæ‰¾åˆ°")
        return None, False


def try_import_pykan():
    """æ™ºèƒ½å¯¼å…¥ pykan (å®˜æ–¹ç‰ˆæœ¬)"""
    try:
        from kan import KAN
        print("âœ… pykan (å®˜æ–¹) å¯¼å…¥æˆåŠŸ")
        return KAN, True
    except ImportError:
        print("âŒ pykan æœªæ‰¾åˆ°")
        return None, False


# æ‰§è¡Œå¯¼å…¥æ£€æŸ¥
KANLinear, EFFICIENT_KAN_AVAILABLE = try_import_efficient_kan()
PyKAN, PYKAN_AVAILABLE = try_import_pykan()


# ========== åŽŸå§‹ MLP_Layersï¼ˆä¿æŒä¸å˜ï¼‰ ==========
class MLP_Layers(torch.nn.Module):
    """åŽŸå§‹çš„ MLP_Layers - å®Œå…¨ä¸å˜ï¼Œä¿è¯ç¨³å®šæ€§"""

    def __init__(self, layers, dnn_layers, drop_rate):
        super(MLP_Layers, self).__init__()
        self.layers = layers
        self.dnn_layers = dnn_layers
        if self.dnn_layers > 0:
            mlp_modules = []
            for idx, (input_size, output_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):
                mlp_modules.append(nn.Dropout(p=drop_rate))
                mlp_modules.append(nn.Linear(input_size, output_size))
                mlp_modules.append(nn.GELU())
            self.mlp_layers = nn.Sequential(*mlp_modules)
            self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def forward(self, x):
        if self.dnn_layers > 0:
            return self.mlp_layers(x)
        else:
            return x


# ========== å¯æŒ‡å®šç‰ˆæœ¬çš„ KAN_Layers ==========
class KAN_Layers(torch.nn.Module):
    """
    å¯æŒ‡å®šç‰ˆæœ¬çš„KAN_Layersï¼Œé€šè¿‡å‚æ•°æŽ§åˆ¶ä½¿ç”¨å“ªä¸ªç‰ˆæœ¬

    ðŸŽ¯ ç‰ˆæœ¬æŽ§åˆ¶ï¼š
    - version='auto': è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç‰ˆæœ¬
    - version='efficient': å¼ºåˆ¶ä½¿ç”¨ Efficient-KAN
    - version='pykan': å¼ºåˆ¶ä½¿ç”¨ PyKAN
    - version='mlp': å¼ºåˆ¶ä½¿ç”¨æ ‡å‡† MLP

    ä½¿ç”¨æ–¹å¼ï¼š
    model = KAN_Layers(layers, dnn_layers, drop_rate, version='efficient')
    """

    def __init__(self, layers, dnn_layers, drop_rate, version='auto'):
        """
        å‚æ•°:
            layers: å±‚ç»´åº¦åˆ—è¡¨
            dnn_layers: ç½‘ç»œå±‚æ•°
            drop_rate: dropoutçŽ‡
            version: 'auto', 'efficient', 'pykan', 'mlp'
        """
        super(KAN_Layers, self).__init__()
        self.layers = layers
        self.dnn_layers = dnn_layers
        self.version = version

        # æ ¹æ®æŒ‡å®šç‰ˆæœ¬åˆå§‹åŒ–
        if version == 'auto':
            self._auto_select_version(drop_rate)
        elif version == 'efficient':
            self._use_efficient_version(drop_rate)
        elif version == 'pykan':
            self._use_pykan_version(drop_rate)
        elif version == 'mlp':
            self._use_mlp_version(drop_rate)
        else:
            print(f"âš ï¸ æœªçŸ¥ç‰ˆæœ¬ '{version}'ï¼Œä½¿ç”¨è‡ªåŠ¨é€‰æ‹©")
            self._auto_select_version(drop_rate)

    def _auto_select_version(self, drop_rate):
        """è‡ªåŠ¨é€‰æ‹©æœ€ä½³ç‰ˆæœ¬"""
        if EFFICIENT_KAN_AVAILABLE:
            print("ðŸŽ¯ è‡ªåŠ¨é€‰æ‹©: Efficient-KAN")
            self._use_efficient_kan(drop_rate)
        elif PYKAN_AVAILABLE:
            print("ðŸŽ¯ è‡ªåŠ¨é€‰æ‹©: PyKAN")
            self._use_pykan(drop_rate)
        else:
            print("ðŸŽ¯ è‡ªåŠ¨é€‰æ‹©: æ ‡å‡† MLP")
            self._use_mlp_fallback(drop_rate)

    def _use_efficient_version(self, drop_rate):
        """ä½¿ç”¨ Efficient-KAN ç‰ˆæœ¬"""
        if EFFICIENT_KAN_AVAILABLE:
            print("âœ… æŒ‡å®šç‰ˆæœ¬: Efficient-KAN")
            self._use_efficient_kan(drop_rate)
        else:
            print("âŒ Efficient-KAN ä¸å¯ç”¨ï¼Œå›žé€€åˆ°æ ‡å‡† MLP")
            self._use_mlp_fallback(drop_rate)

    def _use_pykan_version(self, drop_rate):
        """ä½¿ç”¨ PyKAN ç‰ˆæœ¬"""
        if PYKAN_AVAILABLE:
            print("âœ… æŒ‡å®šç‰ˆæœ¬: PyKAN")
            self._use_pykan(drop_rate)
        else:
            print("âŒ PyKAN ä¸å¯ç”¨ï¼Œå›žé€€åˆ°æ ‡å‡† MLP")
            self._use_mlp_fallback(drop_rate)

    def _use_mlp_version(self, drop_rate):
        """ä½¿ç”¨æ ‡å‡† MLP ç‰ˆæœ¬"""
        print("âœ… æŒ‡å®šç‰ˆæœ¬: æ ‡å‡† MLP")
        self._use_mlp_fallback(drop_rate)

    def _use_efficient_kan(self, drop_rate):
        """ä½¿ç”¨ Efficient-KAN"""
        print("ðŸ”§ ä½¿ç”¨ Efficient-KAN")
        if self.dnn_layers > 0:
            kan_modules = []
            for idx, (input_size, output_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):
                kan_modules.append(nn.Dropout(p=drop_rate))
                kan_modules.append(
                    KANLinear(
                        in_features=input_size,
                        out_features=output_size,
                        grid_size=3,  # æŽ¨èç³»ç»Ÿä¼˜åŒ–å‚æ•°
                        spline_order=2,  # å¹³è¡¡å¤æ‚åº¦å’Œæ€§èƒ½
                        scale_noise=0.05  # ä¿æŒç¨³å®šæ€§
                    )
                )
            self.mlp_layers = nn.Sequential(*kan_modules)
        self.implementation = 'efficient_kan'

    def _use_pykan(self, drop_rate):
        """ä½¿ç”¨ PyKAN"""
        print("ðŸ”§ ä½¿ç”¨ PyKAN")
        if self.dnn_layers > 0:
            self.kan_model = PyKAN(
                width=self.layers,
                grid=3,
                k=2,
                noise_scale=0.05,
                base_fun='silu',
                symbolic_enabled=True,
                seed=42,
                device='cpu'
            )

            if drop_rate > 0:
                self.dropout = nn.Dropout(p=drop_rate)
            else:
                self.dropout = None
        self.implementation = 'pykan'

    def _use_mlp_fallback(self, drop_rate):
        """å›žé€€åˆ°æ ‡å‡†MLP"""
        print("âš ï¸ KANåº“ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†MLP")
        if self.dnn_layers > 0:
            mlp_modules = []
            for idx, (input_size, output_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):
                mlp_modules.append(nn.Dropout(p=drop_rate))
                mlp_modules.append(nn.Linear(input_size, output_size))
                mlp_modules.append(nn.GELU())
            self.mlp_layers = nn.Sequential(*mlp_modules)
            self.apply(self._init_weights)
        self.implementation = 'mlp'

    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def forward(self, x):
        """å‰å‘ä¼ æ’­ - ä¸ŽMLP_Layerså®Œå…¨ç›¸åŒçš„æŽ¥å£"""
        if self.dnn_layers > 0:
            if self.implementation == 'pykan':
                if hasattr(self, 'dropout') and self.dropout is not None:
                    x = self.dropout(x)
                return self.kan_model(x)
            else:
                return self.mlp_layers(x)
        else:
            return x

    def to(self, device):
        """è®¾å¤‡è½¬ç§»"""
        super().to(device)
        if self.implementation == 'pykan' and hasattr(self, 'kan_model'):
            self.kan_model.to(device)
        return self

    # KANç‰¹æœ‰åŠŸèƒ½ï¼ˆå¯é€‰ä½¿ç”¨ï¼‰
    def get_kan_regularization_loss(self, regularize_activation=0.01, regularize_entropy=0.01):
        """èŽ·å–KANæ­£åˆ™åŒ–æŸå¤±"""
        if self.implementation == 'efficient_kan' and self.dnn_layers > 0:
            reg_loss = 0
            for layer in self.mlp_layers:
                if hasattr(layer, 'regularization_loss'):
                    reg_loss += layer.regularization_loss(regularize_activation, regularize_entropy)
            return reg_loss
        return torch.tensor(0.0)

    def plot_kan(self, **kwargs):
        """KANå¯è§†åŒ–ï¼ˆä»…PyKANï¼‰"""
        if self.implementation == 'pykan' and hasattr(self, 'kan_model'):
            return self.kan_model.plot(**kwargs)
        print(f"â„¹ï¸ plot_kan ä»…åœ¨ PyKAN ä¸‹å¯ç”¨ï¼Œå½“å‰ä½¿ç”¨: {self.implementation}")
        return None

    def auto_symbolic(self, **kwargs):
        """è‡ªåŠ¨ç¬¦å·å›žå½’ï¼ˆä»…PyKANï¼‰"""
        if self.implementation == 'pykan' and hasattr(self, 'kan_model'):
            return self.kan_model.auto_symbolic(**kwargs)
        print(f"â„¹ï¸ auto_symbolic ä»…åœ¨ PyKAN ä¸‹å¯ç”¨ï¼Œå½“å‰ä½¿ç”¨: {self.implementation}")
        return None


# class MLP_Layers(torch.nn.Module):
#     def __init__(self, layers, dnn_layers, drop_rate):
#         super(MLP_Layers, self).__init__()
#         self.layers = layers
#         self.dnn_layers = dnn_layers
#         if self.dnn_layers > 0:
#             mlp_modules = []
#             for idx, (input_size, output_size) in enumerate(zip(self.layers[:-1], self.layers[1:])):
#                 mlp_modules.append(nn.Dropout(p=drop_rate))
#                 mlp_modules.append(nn.Linear(input_size, output_size))
#                 mlp_modules.append(nn.GELU())
#             self.mlp_layers = nn.Sequential(*mlp_modules)
#             self.apply(self._init_weights)
#
#     def _init_weights(self, module):
#         if isinstance(module, nn.Embedding):
#             xavier_normal_(module.weight.data)
#         elif isinstance(module, nn.Linear):
#             xavier_normal_(module.weight.data)
#             if module.bias is not None:
#                 constant_(module.bias.data, 0)
#
#     def forward(self, x):
#         if self.dnn_layers > 0:
#             return self.mlp_layers(x)
#         else:
#             return x


class ADD(torch.nn.Module):
    def __init__(self, ):
        super(ADD, self).__init__()

    def forward(self, x, y):
        return x + y


class CAT(torch.nn.Module):
    def __init__(self, input_dim, output_dim, drop_rate):
        super(CAT, self).__init__()
        mlp_modules = []
        mlp_modules.append(nn.Dropout(p=drop_rate))
        mlp_modules.append(nn.Linear(input_dim, output_dim))
        mlp_modules.append(nn.GELU())
        self.mlp_layers = nn.Sequential(*mlp_modules)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def forward(self, x, y):
        con_cat = torch.cat([x, y], 1)
        return self.mlp_layers(con_cat)


class FC_Layers(torch.nn.Module):
    def __init__(self, word_embedding_dim, item_embedding_dim, dnn_layers, drop_rate):
        super(FC_Layers, self).__init__()
        self.dnn_layers = dnn_layers
        self.fc = nn.Linear(word_embedding_dim, item_embedding_dim)
        self.activate = nn.GELU()

        if self.dnn_layers > 0:
            self.mlp_layers = MLP_Layers(layers=[item_embedding_dim] * (self.dnn_layers + 1),
                                         dnn_layers=self.dnn_layers,
                                         drop_rate=drop_rate)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def forward(self, sample_items):
        x = self.activate(self.fc(sample_items))
        if self.dnn_layers > 0:
            return self.mlp_layers(x)
        else:
            return x


class FC_Layers_MLP_KAN(torch.nn.Module):
    """
    ðŸŽ¯ MLPâ†’KAN ä¸¤é˜¶æ®µé™ç»´å±‚ (ç®€åŒ–ç‰ˆ)

    **å®Œå…¨å…¼å®¹FC_LayersæŽ¥å£ï¼Œå¯ç›´æŽ¥æ›¿æ¢ï¼**

    ä½¿ç”¨æ–¹å¼ï¼š
    # åŽŸæ¥ï¼šfc = FC_Layers(4096, 128, 2, 0.1)
    # çŽ°åœ¨ï¼šfc = FC_Layers_MLP_KAN(4096, 128, 2, 0.1)
    """

    def __init__(self, word_embedding_dim, item_embedding_dim, dnn_layers, drop_rate):
        super(FC_Layers_MLP_KAN, self).__init__()

        self.dnn_layers = dnn_layers

        # è®¡ç®—ä¸­é—´ç»´åº¦
        intermediate_dim = int((word_embedding_dim * item_embedding_dim) ** 0.5)
        intermediate_dim = ((intermediate_dim + 63) // 64) * 64
        intermediate_dim = max(intermediate_dim, item_embedding_dim * 2)

        # é˜¶æ®µ1ï¼šMLPé™ç»´
        self.mlp_stage = nn.Sequential(
            nn.Dropout(drop_rate),
            nn.Linear(word_embedding_dim, intermediate_dim),
            nn.GELU()
        )

        # é˜¶æ®µ2ï¼šKANé™ç»´ (è‡ªåŠ¨å›žé€€)
        if EFFICIENT_KAN_AVAILABLE:
            self.kan_stage = nn.Sequential(
                nn.Dropout(drop_rate),
                KANLinear(intermediate_dim, item_embedding_dim, grid_size=3, spline_order=2)
            )
            self.use_pykan = False
            print(f"âœ… ä½¿ç”¨ Efficient-KAN: {word_embedding_dim}â†’{intermediate_dim}â†’{item_embedding_dim}")
        elif PYKAN_AVAILABLE:
            self.kan_model = KAN(width=[intermediate_dim, item_embedding_dim], grid=3, k=2)
            self.kan_dropout = nn.Dropout(drop_rate) if drop_rate > 0 else None
            self.use_pykan = True
            print(f"âœ… ä½¿ç”¨ PyKAN: {word_embedding_dim}â†’{intermediate_dim}â†’{item_embedding_dim}")
        else:
            self.kan_stage = nn.Sequential(
                nn.Dropout(drop_rate),
                nn.Linear(intermediate_dim, item_embedding_dim),
                nn.GELU()
            )
            self.use_pykan = False
            print(f"âš ï¸ KANä¸å¯ç”¨ï¼Œä½¿ç”¨MLP: {word_embedding_dim}â†’{intermediate_dim}â†’{item_embedding_dim}")

        # é˜¶æ®µ3ï¼šåŽç»­MLPå±‚
        if self.dnn_layers > 0:
            mlp_modules = []
            layers = [item_embedding_dim] * (self.dnn_layers + 1)
            for idx, (input_size, output_size) in enumerate(zip(layers[:-1], layers[1:])):
                mlp_modules.append(nn.Dropout(p=drop_rate))
                mlp_modules.append(nn.Linear(input_size, output_size))
                mlp_modules.append(nn.GELU())
            self.post_mlp = nn.Sequential(*mlp_modules)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def forward(self, sample_items):
        # é˜¶æ®µ1ï¼šMLPé™ç»´
        x = self.mlp_stage(sample_items)

        # é˜¶æ®µ2ï¼šKANé™ç»´
        if self.use_pykan:
            if hasattr(self, 'kan_dropout') and self.kan_dropout is not None:
                x = self.kan_dropout(x)
            x = self.kan_model(x)
        else:
            x = self.kan_stage(x)

        # é˜¶æ®µ3ï¼šåŽç»­å¤„ç†
        if self.dnn_layers > 0:
            x = self.post_mlp(x)

        return x

    def to(self, device):
        super().to(device)
        if hasattr(self, 'use_pykan') and self.use_pykan and hasattr(self, 'kan_model'):
            self.kan_model.to(device)
        return self


class FC_Layers_KAN(torch.nn.Module):
    """
    ðŸŽ¯ çº¯KANé™ç»´å±‚ (ç®€åŒ–ç‰ˆ)

    **å®Œå…¨å…¼å®¹FC_LayersæŽ¥å£ï¼Œç›´æŽ¥KANé™ç»´4096â†’128ï¼**

    ä½¿ç”¨æ–¹å¼ï¼š
    # åŽŸæ¥ï¼šfc = FC_Layers(4096, 128, 2, 0.1)
    # çŽ°åœ¨ï¼šfc = FC_Layers_KAN(4096, 128, 2, 0.1)
    """

    def __init__(self, word_embedding_dim, item_embedding_dim, dnn_layers, drop_rate):
        super(FC_Layers_KAN, self).__init__()

        self.dnn_layers = dnn_layers

        # ç›´æŽ¥KANé™ç»´ï¼š4096â†’128
        if EFFICIENT_KAN_AVAILABLE:
            self.fc = nn.Sequential(
                nn.Dropout(drop_rate),
                KANLinear(
                    in_features=word_embedding_dim,
                    out_features=item_embedding_dim,
                    grid_size=5,  # å¤§é™ç»´æ¯”ä¾‹ï¼Œç”¨æ›´å¤§çš„grid
                    spline_order=3,  # æ›´é«˜é˜¶æ ·æ¡ï¼Œå¢žå¼ºè¡¨è¾¾èƒ½åŠ›
                    scale_noise=0.1,
                    scale_base=1.0,
                    scale_spline=1.0
                )
            )
            self.use_pykan = False
            print(f"âœ… ä½¿ç”¨ Efficient-KAN ç›´æŽ¥é™ç»´: {word_embedding_dim}â†’{item_embedding_dim}")

        elif PYKAN_AVAILABLE:
            self.kan_model = KAN(
                width=[word_embedding_dim, item_embedding_dim],
                grid=5,  # å¤§é™ç»´æ¯”ä¾‹ï¼Œç”¨æ›´å¤§çš„grid
                k=3,  # æ›´é«˜é˜¶æ ·æ¡
                noise_scale=0.1,
                base_fun='silu'
            )
            self.kan_dropout = nn.Dropout(drop_rate) if drop_rate > 0 else None
            self.use_pykan = True
            print(f"âœ… ä½¿ç”¨ PyKAN ç›´æŽ¥é™ç»´: {word_embedding_dim}â†’{item_embedding_dim}")

        else:
            # å›žé€€åˆ°æ ‡å‡†MLPï¼ˆä¸ŽåŽŸFC_Layersç›¸åŒï¼‰
            self.fc = nn.Linear(word_embedding_dim, item_embedding_dim)
            self.activate = nn.GELU()
            self.use_pykan = False
            print(f"âš ï¸ KANä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†MLP: {word_embedding_dim}â†’{item_embedding_dim}")

        # åŽç»­MLPå±‚ï¼ˆä¸ŽåŽŸFC_Layerså®Œå…¨ç›¸åŒï¼‰
        if self.dnn_layers > 0:
            mlp_modules = []
            layers = [item_embedding_dim] * (self.dnn_layers + 1)
            for idx, (input_size, output_size) in enumerate(zip(layers[:-1], layers[1:])):
                mlp_modules.append(nn.Dropout(p=drop_rate))
                mlp_modules.append(nn.Linear(input_size, output_size))
                mlp_modules.append(nn.GELU())
            self.mlp_layers = nn.Sequential(*mlp_modules)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def forward(self, sample_items):
        # KANç›´æŽ¥é™ç»´
        if self.use_pykan:
            if hasattr(self, 'kan_dropout') and self.kan_dropout is not None:
                x = self.kan_dropout(sample_items)
            else:
                x = sample_items
            x = self.kan_model(x)
        else:
            if hasattr(self, 'activate'):  # æ ‡å‡†MLPå›žé€€
                x = self.activate(self.fc(sample_items))
            else:  # Efficient-KAN
                x = self.fc(sample_items)

        # åŽç»­MLPå±‚
        if self.dnn_layers > 0:
            return self.mlp_layers(x)
        else:
            return x

    def to(self, device):
        super().to(device)
        if hasattr(self, 'use_pykan') and self.use_pykan and hasattr(self, 'kan_model'):
            self.kan_model.to(device)
        return self


# ðŸŽ¯ æ–¹æ¡ˆ1: çº¯KANç›´æŽ¥é™ç»´ (å·²æœ‰æ–¹æ¡ˆ)
class FC_Layers_KAN_Direct(torch.nn.Module):
    """çº¯KANç›´æŽ¥é™ç»´: 4096â†’128 ä¸€æ­¥åˆ°ä½"""

    def __init__(self, word_embedding_dim, item_embedding_dim, dnn_layers, drop_rate):
        super(FC_Layers_KAN_Direct, self).__init__()
        self.dnn_layers = dnn_layers

        if EFFICIENT_KAN_AVAILABLE:
            self.fc = nn.Sequential(
                nn.Dropout(drop_rate),
                KANLinear(word_embedding_dim, item_embedding_dim, grid_size=5, spline_order=3)
            )
            self.use_pykan = False
            print(f"âœ… [æ–¹æ¡ˆ1] Efficient-KANç›´æŽ¥é™ç»´: {word_embedding_dim}â†’{item_embedding_dim}")
        elif PYKAN_AVAILABLE:
            self.kan_model = KAN(width=[word_embedding_dim, item_embedding_dim], grid=5, k=3)
            self.kan_dropout = nn.Dropout(drop_rate) if drop_rate > 0 else None
            self.use_pykan = True
            print(f"âœ… [æ–¹æ¡ˆ1] PyKANç›´æŽ¥é™ç»´: {word_embedding_dim}â†’{item_embedding_dim}")
        else:
            self.fc = nn.Linear(word_embedding_dim, item_embedding_dim)
            self.activate = nn.GELU()
            self.use_pykan = False
            print(f"âš ï¸ [æ–¹æ¡ˆ1] KANä¸å¯ç”¨ï¼Œä½¿ç”¨MLP: {word_embedding_dim}â†’{item_embedding_dim}")

        if self.dnn_layers > 0:
            mlp_modules = []
            layers = [item_embedding_dim] * (self.dnn_layers + 1)
            for idx, (input_size, output_size) in enumerate(zip(layers[:-1], layers[1:])):
                mlp_modules.append(nn.Dropout(p=drop_rate))
                mlp_modules.append(nn.Linear(input_size, output_size))
                mlp_modules.append(nn.GELU())
            self.mlp_layers = nn.Sequential(*mlp_modules)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def forward(self, sample_items):
        if self.use_pykan:
            if hasattr(self, 'kan_dropout') and self.kan_dropout is not None:
                x = self.kan_dropout(sample_items)
            else:
                x = sample_items
            x = self.kan_model(x)
        else:
            if hasattr(self, 'activate'):
                x = self.activate(self.fc(sample_items))
            else:
                x = self.fc(sample_items)

        if self.dnn_layers > 0:
            return self.mlp_layers(x)
        return x


# ðŸŽ¯ æ–¹æ¡ˆ2: æ¸è¿›å¼MLPé™ç»´ (ç¨³å®šä¿å®ˆ)
class FC_Layers_Progressive_MLP(torch.nn.Module):
    """æ¸è¿›å¼MLPé™ç»´: 4096â†’2048â†’1024â†’512â†’128 é€æ­¥é™ç»´"""

    def __init__(self, word_embedding_dim, item_embedding_dim, dnn_layers, drop_rate):
        super(FC_Layers_Progressive_MLP, self).__init__()
        self.dnn_layers = dnn_layers

        # è®¡ç®—æ¸è¿›é™ç»´çš„ä¸­é—´ç»´åº¦
        num_stages = 4  # 4ä¸ªé˜¶æ®µ
        dims = [word_embedding_dim]
        for i in range(1, num_stages):
            dim = int(word_embedding_dim * (0.5 ** i))  # æ¯é˜¶æ®µå‡åŠ
            dim = max(dim, item_embedding_dim)  # ä¸å°äºŽç›®æ ‡ç»´åº¦
            dims.append(dim)
        dims.append(item_embedding_dim)

        # åŽ»é‡å¹¶ä¿æŒé€’å‡
        dims = sorted(list(set(dims)), reverse=True)
        if dims[-1] != item_embedding_dim:
            dims.append(item_embedding_dim)

        # æž„å»ºæ¸è¿›MLP
        progressive_modules = []
        for i in range(len(dims) - 1):
            progressive_modules.extend([
                nn.Dropout(drop_rate),
                nn.Linear(dims[i], dims[i + 1]),
                nn.GELU()
            ])

        self.progressive_mlp = nn.Sequential(*progressive_modules)

        if self.dnn_layers > 0:
            mlp_modules = []
            layers = [item_embedding_dim] * (self.dnn_layers + 1)
            for idx, (input_size, output_size) in enumerate(zip(layers[:-1], layers[1:])):
                mlp_modules.append(nn.Dropout(p=drop_rate))
                mlp_modules.append(nn.Linear(input_size, output_size))
                mlp_modules.append(nn.GELU())
            self.mlp_layers = nn.Sequential(*mlp_modules)

        print(f"âœ… [æ–¹æ¡ˆ2] æ¸è¿›å¼MLP: {word_embedding_dim}â†’{dims[1:-1]}â†’{item_embedding_dim}")
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def forward(self, sample_items):
        x = self.progressive_mlp(sample_items)
        if self.dnn_layers > 0:
            return self.mlp_layers(x)
        return x


# ðŸŽ¯ æ–¹æ¡ˆ3: KANç‰¹å¾æå– + MLPé™ç»´
class FC_Layers_KAN_Feature_MLP(torch.nn.Module):
    """KANç‰¹å¾æå–+MLPé™ç»´: 4096â†’[KAN]â†’512â†’[MLP]â†’128"""

    def __init__(self, word_embedding_dim, item_embedding_dim, dnn_layers, drop_rate):
        super(FC_Layers_KAN_Feature_MLP, self).__init__()
        self.dnn_layers = dnn_layers

        # ç‰¹å¾æå–ç»´åº¦ (ä¿æŒè¾ƒé«˜ç»´åº¦è¿›è¡Œç‰¹å¾æå–)
        feature_dim = max(word_embedding_dim // 8, 512)  # é€šå¸¸512ç»´

        # é˜¶æ®µ1: KANç‰¹å¾æå–
        if EFFICIENT_KAN_AVAILABLE:
            self.feature_kan = nn.Sequential(
                nn.Dropout(drop_rate),
                KANLinear(word_embedding_dim, feature_dim, grid_size=3, spline_order=2)
            )
            self.use_pykan = False
            print(f"âœ… [æ–¹æ¡ˆ3] Efficient-KANç‰¹å¾æå–: {word_embedding_dim}â†’{feature_dim}")
        elif PYKAN_AVAILABLE:
            self.kan_model = KAN(width=[word_embedding_dim, feature_dim], grid=3, k=2)
            self.kan_dropout = nn.Dropout(drop_rate) if drop_rate > 0 else None
            self.use_pykan = True
            print(f"âœ… [æ–¹æ¡ˆ3] PyKANç‰¹å¾æå–: {word_embedding_dim}â†’{feature_dim}")
        else:
            self.feature_kan = nn.Sequential(
                nn.Dropout(drop_rate),
                nn.Linear(word_embedding_dim, feature_dim),
                nn.GELU()
            )
            self.use_pykan = False
            print(f"âš ï¸ [æ–¹æ¡ˆ3] MLPç‰¹å¾æå–: {word_embedding_dim}â†’{feature_dim}")

        # é˜¶æ®µ2: MLPé™ç»´
        self.dimension_mlp = nn.Sequential(
            nn.Dropout(drop_rate),
            nn.Linear(feature_dim, item_embedding_dim),
            nn.GELU()
        )

        if self.dnn_layers > 0:
            mlp_modules = []
            layers = [item_embedding_dim] * (self.dnn_layers + 1)
            for idx, (input_size, output_size) in enumerate(zip(layers[:-1], layers[1:])):
                mlp_modules.append(nn.Dropout(p=drop_rate))
                mlp_modules.append(nn.Linear(input_size, output_size))
                mlp_modules.append(nn.GELU())
            self.mlp_layers = nn.Sequential(*mlp_modules)

        print(f"       MLPé™ç»´: {feature_dim}â†’{item_embedding_dim}")
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def forward(self, sample_items):
        # é˜¶æ®µ1: KANç‰¹å¾æå–
        if self.use_pykan:
            if hasattr(self, 'kan_dropout') and self.kan_dropout is not None:
                x = self.kan_dropout(sample_items)
            else:
                x = sample_items
            x = self.kan_model(x)
        else:
            x = self.feature_kan(sample_items)

        # é˜¶æ®µ2: MLPé™ç»´
        x = self.dimension_mlp(x)

        if self.dnn_layers > 0:
            return self.mlp_layers(x)
        return x


# ðŸŽ¯ æ–¹æ¡ˆ4: åŒKANä¸²è” (åˆ†ç¦»å¼)
class FC_Layers_Dual_KAN(torch.nn.Module):
    """åŒKANä¸²è”: 4096â†’[KAN1]â†’1024â†’[KAN2]â†’128 (çœŸæ­£çš„ä¸¤ä¸ªç‹¬ç«‹KAN)"""

    def __init__(self, word_embedding_dim, item_embedding_dim, dnn_layers, drop_rate):
        super(FC_Layers_Dual_KAN, self).__init__()
        self.dnn_layers = dnn_layers

        # ä¸­é—´ç»´åº¦
        intermediate_dim = max(word_embedding_dim // 4, 1024)  # é€šå¸¸1024ç»´

        # ðŸ”§ æ–¹å¼1: Efficient-KANåŒå±‚
        if EFFICIENT_KAN_AVAILABLE:
            # ç¬¬ä¸€ä¸ªKAN: ç²—é™ç»´ (4096â†’1024)
            self.kan1 = nn.Sequential(
                nn.Dropout(drop_rate),
                KANLinear(
                    word_embedding_dim, intermediate_dim,
                    grid_size=3,  # ç²—é™ç»´ç”¨è¾ƒå°ç½‘æ ¼
                    spline_order=2,  # äºŒæ¬¡æ ·æ¡
                    scale_noise=0.1
                )
            )
            # ç¬¬äºŒä¸ªKAN: ç²¾ç»†é™ç»´ (1024â†’128)
            self.kan2 = nn.Sequential(
                nn.Dropout(drop_rate),
                KANLinear(
                    intermediate_dim, item_embedding_dim,
                    grid_size=5,  # ç²¾ç»†é™ç»´ç”¨è¾ƒå¤§ç½‘æ ¼
                    spline_order=3,  # ä¸‰æ¬¡æ ·æ¡ï¼Œæ›´å¼ºè¡¨è¾¾åŠ›
                    scale_noise=0.1
                )
            )
            self.kan_type = 'efficient_dual'
            print(f"âœ… [æ–¹æ¡ˆ4] åŒEfficient-KANåˆ†ç¦»: {word_embedding_dim}â†’{intermediate_dim}â†’{item_embedding_dim}")

        # ðŸ”§ æ–¹å¼2: PyKANåŒå±‚ (åˆ†åˆ«åˆ›å»º)
        elif PYKAN_AVAILABLE:
            # ç¬¬ä¸€ä¸ªPyKAN: ç²—é™ç»´
            self.kan1_model = KAN(
                width=[word_embedding_dim, intermediate_dim],
                grid=3, k=2,
                noise_scale=0.1,
                base_fun='silu'
            )
            # ç¬¬äºŒä¸ªPyKAN: ç²¾ç»†é™ç»´
            self.kan2_model = KAN(
                width=[intermediate_dim, item_embedding_dim],
                grid=5, k=3,
                noise_scale=0.1,
                base_fun='silu'
            )
            self.kan1_dropout = nn.Dropout(drop_rate) if drop_rate > 0 else None
            self.kan2_dropout = nn.Dropout(drop_rate) if drop_rate > 0 else None
            self.kan_type = 'pykan_dual'
            print(f"âœ… [æ–¹æ¡ˆ4] åŒPyKANåˆ†ç¦»: {word_embedding_dim}â†’{intermediate_dim}â†’{item_embedding_dim}")

        # ðŸ”§ æ–¹å¼3: æ··åˆæ–¹å¼ (ä¸€ä¸ªç”¨KANï¼Œä¸€ä¸ªç”¨MLP)
        elif EFFICIENT_KAN_AVAILABLE or PYKAN_AVAILABLE:
            if EFFICIENT_KAN_AVAILABLE:
                # KANåšç‰¹å¾æå–ï¼ŒMLPåšé™ç»´
                self.kan1 = nn.Sequential(
                    nn.Dropout(drop_rate),
                    KANLinear(word_embedding_dim, intermediate_dim, grid_size=3, spline_order=2)
                )
                self.kan_type = 'efficient_mlp_hybrid'
            else:
                # PyKANåšç‰¹å¾æå–
                self.kan1_model = KAN(
                    width=[word_embedding_dim, intermediate_dim],
                    grid=3, k=2
                )
                self.kan1_dropout = nn.Dropout(drop_rate) if drop_rate > 0 else None
                self.kan_type = 'pykan_mlp_hybrid'

            # MLPåšæœ€ç»ˆé™ç»´
            self.kan2 = nn.Sequential(
                nn.Dropout(drop_rate),
                nn.Linear(intermediate_dim, item_embedding_dim),
                nn.GELU()
            )
            print(f"âœ… [æ–¹æ¡ˆ4] KAN+MLPæ··åˆ: {word_embedding_dim}â†’{intermediate_dim}â†’{item_embedding_dim}")

        # ðŸ”§ æ–¹å¼4: åŒMLPå›žé€€
        else:
            self.kan1 = nn.Sequential(
                nn.Dropout(drop_rate),
                nn.Linear(word_embedding_dim, intermediate_dim),
                nn.GELU()
            )
            self.kan2 = nn.Sequential(
                nn.Dropout(drop_rate),
                nn.Linear(intermediate_dim, item_embedding_dim),
                nn.GELU()
            )
            self.kan_type = 'mlp_dual'
            print(f"âš ï¸ [æ–¹æ¡ˆ4] åŒMLPå›žé€€: {word_embedding_dim}â†’{intermediate_dim}â†’{item_embedding_dim}")

        if self.dnn_layers > 0:
            mlp_modules = []
            layers = [item_embedding_dim] * (self.dnn_layers + 1)
            for idx, (input_size, output_size) in enumerate(zip(layers[:-1], layers[1:])):
                mlp_modules.append(nn.Dropout(p=drop_rate))
                mlp_modules.append(nn.Linear(input_size, output_size))
                mlp_modules.append(nn.GELU())
            self.mlp_layers = nn.Sequential(*mlp_modules)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def forward(self, sample_items):
        # æ ¹æ®KANç±»åž‹é€‰æ‹©å‰å‘è·¯å¾„
        if self.kan_type == 'efficient_dual':
            # åŒEfficient-KAN
            x = self.kan1(sample_items)
            x = self.kan2(x)

        elif self.kan_type == 'pykan_dual':
            # åŒPyKANåˆ†ç¦»
            if self.kan1_dropout is not None:
                x = self.kan1_dropout(sample_items)
            else:
                x = sample_items
            x = self.kan1_model(x)

            if self.kan2_dropout is not None:
                x = self.kan2_dropout(x)
            x = self.kan2_model(x)

        elif self.kan_type == 'efficient_mlp_hybrid':
            # Efficient-KAN + MLP
            x = self.kan1(sample_items)
            x = self.kan2(x)

        elif self.kan_type == 'pykan_mlp_hybrid':
            # PyKAN + MLP
            if self.kan1_dropout is not None:
                x = self.kan1_dropout(sample_items)
            else:
                x = sample_items
            x = self.kan1_model(x)
            x = self.kan2(x)

        else:  # mlp_dual
            # åŒMLPå›žé€€
            x = self.kan1(sample_items)
            x = self.kan2(x)

        if self.dnn_layers > 0:
            return self.mlp_layers(x)
        return x

    def to(self, device):
        super().to(device)
        if hasattr(self, 'kan1_model'):
            self.kan1_model.to(device)
        if hasattr(self, 'kan2_model'):
            self.kan2_model.to(device)
        return self


# ðŸŽ¯ æ–¹æ¡ˆ5: è½»é‡çº§KAN (å¿«é€Ÿè®­ç»ƒ)
class FC_Layers_Lightweight_KAN(torch.nn.Module):
    """è½»é‡çº§KAN: å‚æ•°å°‘ã€è®­ç»ƒå¿«"""

    def __init__(self, word_embedding_dim, item_embedding_dim, dnn_layers, drop_rate):
        super(FC_Layers_Lightweight_KAN, self).__init__()
        self.dnn_layers = dnn_layers

        if EFFICIENT_KAN_AVAILABLE:
            self.fc = nn.Sequential(
                nn.Dropout(drop_rate),
                KANLinear(
                    word_embedding_dim, item_embedding_dim,
                    grid_size=2,  # æ›´å°ç½‘æ ¼ï¼Œå‡å°‘å‚æ•°
                    spline_order=1,  # çº¿æ€§æ ·æ¡ï¼Œè®­ç»ƒæ›´å¿«
                    scale_noise=0.05  # è¾ƒå°‘å™ªå£°
                )
            )
            self.use_pykan = False
            print(f"âœ… [æ–¹æ¡ˆ5] è½»é‡çº§Efficient-KAN: {word_embedding_dim}â†’{item_embedding_dim}")
        elif PYKAN_AVAILABLE:
            self.kan_model = KAN(
                width=[word_embedding_dim, item_embedding_dim],
                grid=2,  # æ›´å°ç½‘æ ¼
                k=1,  # çº¿æ€§æ ·æ¡
                noise_scale=0.05
            )
            self.kan_dropout = nn.Dropout(drop_rate) if drop_rate > 0 else None
            self.use_pykan = True
            print(f"âœ… [æ–¹æ¡ˆ5] è½»é‡çº§PyKAN: {word_embedding_dim}â†’{item_embedding_dim}")
        else:
            self.fc = nn.Linear(word_embedding_dim, item_embedding_dim)
            self.activate = nn.GELU()
            self.use_pykan = False
            print(f"âš ï¸ [æ–¹æ¡ˆ5] æ ‡å‡†MLP: {word_embedding_dim}â†’{item_embedding_dim}")

        if self.dnn_layers > 0:
            mlp_modules = []
            layers = [item_embedding_dim] * (self.dnn_layers + 1)
            for idx, (input_size, output_size) in enumerate(zip(layers[:-1], layers[1:])):
                mlp_modules.append(nn.Dropout(p=drop_rate))
                mlp_modules.append(nn.Linear(input_size, output_size))
                mlp_modules.append(nn.GELU())
            self.mlp_layers = nn.Sequential(*mlp_modules)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def forward(self, sample_items):
        if self.use_pykan:
            if hasattr(self, 'kan_dropout') and self.kan_dropout is not None:
                x = self.kan_dropout(sample_items)
            else:
                x = sample_items
            x = self.kan_model(x)
        else:
            if hasattr(self, 'activate'):
                x = self.activate(self.fc(sample_items))
            else:
                x = self.fc(sample_items)

        if self.dnn_layers > 0:
            return self.mlp_layers(x)
        return x


class User_Encoder(torch.nn.Module):
    def __init__(self, item_num, max_seq_len, item_dim, num_attention_heads, dropout, n_layers):
        super(User_Encoder, self).__init__()
        self.transformer_encoder = TransformerEncoder(n_vocab=item_num, n_position=max_seq_len,
                                                      d_model=item_dim, n_heads=num_attention_heads,
                                                      dropout=dropout, n_layers=n_layers)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Embedding):
            xavier_normal_(module.weight.data)
        elif isinstance(module, nn.Linear):
            xavier_normal_(module.weight.data)
            if module.bias is not None:
                constant_(module.bias.data, 0)

    def forward(self, input_embs, log_mask, local_rank):
        att_mask = (log_mask != 0)
        att_mask = att_mask.unsqueeze(1).unsqueeze(2)  # torch.bool
        att_mask = torch.tril(att_mask.expand((-1, -1, log_mask.size(-1), -1))).to(local_rank)
        att_mask = torch.where(att_mask, 0., -1e9)
        return self.transformer_encoder(input_embs, log_mask, att_mask)


class Text_Encoder_mean(torch.nn.Module):
    def __init__(self,
                 bert_model,
                 item_embedding_dim,
                 word_embedding_dim):
        super(Text_Encoder_mean, self).__init__()
        self.bert_model = bert_model
        # self.fc = nn.Linear(word_embedding_dim, item_embedding_dim)
        # self.activate = nn.GELU()

    def forward(self, text):
        batch_size, num_words = text.shape
        num_words = num_words // 2
        text_ids = torch.narrow(text, 1, 0, num_words)
        text_attmask = torch.narrow(text, 1, num_words, num_words)
        hidden_states = self.bert_model(input_ids=text_ids, attention_mask=text_attmask)[0]
        input_mask_expanded = text_attmask.unsqueeze(-1).expand(hidden_states.size()).float()
        mean_output = torch.sum(hidden_states * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1),
                                                                                      min=1e-9)
        return mean_output
        # mean_output = self.fc(mean_output)
        # return self.activate(mean_output)


class Text_Encoder(torch.nn.Module):
    def __init__(self,
                 bert_model,
                 item_embedding_dim,
                 word_embedding_dim):
        super(Text_Encoder, self).__init__()
        self.bert_model = bert_model
        # self.fc = nn.Linear(word_embedding_dim, item_embedding_dim)
        # self.activate = nn.GELU()

    def forward(self, text):
        batch_size, num_words = text.shape
        num_words = num_words // 2
        text_ids = torch.narrow(text, 1, 0, num_words)
        text_attmask = torch.narrow(text, 1, num_words, num_words)
        hidden_states = self.bert_model(input_ids=text_ids, attention_mask=text_attmask)[0]
        return hidden_states[:, 0]
        # cls = self.fc(hidden_states[:, 0])
        # return self.activate(cls)


class Bert_Encoder(torch.nn.Module):
    def __init__(self, args, bert_model):
        super(Bert_Encoder, self).__init__()
        self.args = args
        self.attributes2length = {
            'title': args.num_words_title * 2,
            'abstract': args.num_words_abstract * 2,
            'body': args.num_words_body * 2
        }
        for key in list(self.attributes2length.keys()):
            if key not in args.news_attributes:
                self.attributes2length[key] = 0

        self.attributes2start = {
            key: sum(
                list(self.attributes2length.values())
                [:list(self.attributes2length.keys()).index(key)]
            )
            for key in self.attributes2length.keys()
        }

        assert len(args.news_attributes) > 0
        text_encoders_candidates = ['title', 'abstract', 'body']

        if 'opt' in args.bert_model_load:
            self.text_encoders = nn.ModuleDict({
                'title': Text_Encoder_mean(bert_model, args.embedding_dim, args.word_embedding_dim)
            })
        else:
            self.text_encoders = nn.ModuleDict({
                'title': Text_Encoder(bert_model, args.embedding_dim, args.word_embedding_dim)
            })

        self.newsname = [name for name in set(args.news_attributes) & set(text_encoders_candidates)]

    def forward(self, news):
        text_vectors = [
            self.text_encoders['title'](
                torch.narrow(news, 1, self.attributes2start[name], self.attributes2length[name]))
            for name in self.newsname
        ]
        if len(text_vectors) == 1:
            final_news_vector = text_vectors[0]
        else:
            final_news_vector = torch.mean(torch.stack(text_vectors, dim=1), dim=1)
        return final_news_vector

